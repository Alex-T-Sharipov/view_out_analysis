{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "s1_exp1_impressions_df = pd.read_csv(\"1stSession_EXP1 Impressions.csv\")\n",
    "s1_exp2_impressions_df = pd.read_csv(\"1stSession_EXP2 Impressions.csv\")\n",
    "s1_exp1_background_df = pd.read_csv(\"1stSession_EXP1 BackgroundEnvironment.csv\")\n",
    "s1_exp2_background_df = pd.read_csv(\"1stSession_EXP2 BackgroundEnvironment.csv\")\n",
    "\n",
    "\n",
    "s2_exp1_impressions_df = pd.read_csv(\"2ndSession_EXP1 Impressions.csv\")\n",
    "s2_exp2_impressions_df = pd.read_csv(\"2ndSession_EXP2 Impressions.csv\")\n",
    "s2_exp1_background_df = pd.read_csv(\"2ndSession_EXP1 BackgroundEnvironment.csv\")\n",
    "s2_exp2_background_df = pd.read_csv(\"2stSession_EXP2 BackgroundEnvironment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined the data from the 2 experimental sessions\n",
    "\n",
    "combined_e1 = pd.concat(\n",
    "    [s1_exp1_impressions_df, s2_exp1_impressions_df], ignore_index=True\n",
    ")\n",
    "combined_e2 = pd.concat(\n",
    "    [s1_exp2_impressions_df, s2_exp2_impressions_df], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by scene number and scene type\n",
    "\n",
    "groups_e1 = (\n",
    "    combined_e1.groupby([\"Scene Number\", \"Scene Type\"])[\"Sentences Overall\"]\n",
    "    .agg(lambda x: \";;;\\n\".join(x))\n",
    "    .reset_index()\n",
    ")\n",
    "groups_e2 = (\n",
    "    combined_e2.groupby([\"Scene Number\", \"Scene Type\"])[\"Sentences Overall\"]\n",
    "    .agg(lambda x: \";;;\\n\".join(x))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(grouped_dataframe, tfidf_vectorizer, name):\n",
    "    \"\"\" Find the top 10 words within each group in the dataframe\"\"\"\n",
    "    with open(name, \"w\") as file:\n",
    "        file.write(\"Scene Number, Scene Type, Top words\\n\")\n",
    "        for index, row in grouped_dataframe.iterrows():\n",
    "            tfidf_result = tfidf_vectorizer.fit_transform(\n",
    "                row[\"Sentences Overall\"].split(\";;;\\n\")\n",
    "            )\n",
    "            nmf_result = NMF(n_components=1, random_state=1).fit(tfidf_result)\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            for index, topic in enumerate(nmf_result.components_):\n",
    "                top_words = \" \".join(\n",
    "                    [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    "                )\n",
    "                file.write(f\"{row['Scene Number']}, {row['Scene Type']}, {top_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, stop_words=\"english\", ngram_range=(1, 1)\n",
    ")\n",
    "preprocess(groups_e1, tfidf_vectorizer, \"exp_1.csv\")\n",
    "preprocess(groups_e2, tfidf_vectorizer, \"exp_2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
